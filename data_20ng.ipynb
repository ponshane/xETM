{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "print('reading data...')\n",
    "train_data = fetch_20newsgroups(subset='train')\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "# retrieving data\n",
    "# the following regular expression would strip any space and change line symbol (/n)\n",
    "# note: \\w = [a-zA-Z0-9_] \n",
    "init_docs_tr = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', train_data.data[doc]) for doc in range(len(train_data.data))]\n",
    "init_docs_ts = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', test_data.data[doc]) for doc in range(len(test_data.data))]\n",
    "\n",
    "def contains_punctuation(w):\n",
    "    return any(char in string.punctuation for char in w)\n",
    "\n",
    "def contains_numeric(w):\n",
    "    return any(char.isdigit() for char in w)\n",
    "\n",
    "# pre-processing\n",
    "init_docs = init_docs_tr + init_docs_ts\n",
    "# remove punctuation & lowerize characters\n",
    "init_docs = [[w.lower() for w in init_docs[doc] if not contains_punctuation(w)] for doc in range(len(init_docs))]\n",
    "# remove numeric\n",
    "init_docs = [[w for w in init_docs[doc] if not contains_numeric(w)] for doc in range(len(init_docs))]\n",
    "# remove single character, e.g., \"a\", \"b\"\n",
    "init_docs = [[w for w in init_docs[doc] if len(w)>1] for doc in range(len(init_docs))]\n",
    "# unnest the nested list into format [\"tokens of first document 1\". \"......\"]\n",
    "init_docs = [\" \".join(init_docs[doc]) for doc in range(len(init_docs))] # len(init_docs) = 18846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n",
      "== After processing ==\n",
      "\n",
      "from guykuo carson washington edu guy kuo subject si clock poll final call summary final call for si clock reports keywords si acceleration clock upgrade article shelley organization university of washington lines nntp posting host carson washington edu fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll please send brief message detailing your experiences with the procedure top speed attained cpu rated speed add on cards and adapters heat sinks hour of usage per day floppy disk functionality with and floppies are especially requested will be summarizing in the next two days so please add to the network knowledge base if you have done the clock upgrade and answered this poll thanks guy kuo guykuo washington edu\n"
     ]
    }
   ],
   "source": [
    "# see an example of preprocessing\n",
    "print(train_data.data[1])\n",
    "print(\"== After processing ==\\n\")\n",
    "print(init_docs[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Build Vocabulary Mapping (id2word, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting document frequency of words...\n"
     ]
    }
   ],
   "source": [
    "# Create count vectorizer\n",
    "# Maximum / minimum document frequency\n",
    "max_df = 0.7\n",
    "min_df = 10  # choose desired value for min_df\n",
    "\n",
    "print('counting document frequency of words...')\n",
    "cvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=None)\n",
    "# cvz is a document-term matrix\n",
    "# cvz is a 18846x19148 sparse matrix\n",
    "cvz = cvectorizer.fit_transform(init_docs).sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the vocabulary...\n",
      "  initial vocabulary size: 19148\n",
      "  vocabulary size after removing stopwords from list: 18677\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "print('building the vocabulary...')\n",
    "sum_counts = cvz.sum(axis=0) # calculate (axis=0) frequency of each term\n",
    "v_size = sum_counts.shape[1]\n",
    "sum_counts_np = np.zeros(v_size, dtype=int)\n",
    "for v in range(v_size):\n",
    "    sum_counts_np[v] = sum_counts[0,v]\n",
    "# word2id, a dicitonary maps word to id\n",
    "word2id = dict([(w, cvectorizer.vocabulary_.get(w)) for w in cvectorizer.vocabulary_])\n",
    "# id2word, a dicitonary maps id to word\n",
    "id2word = dict([(cvectorizer.vocabulary_.get(w), w) for w in cvectorizer.vocabulary_])\n",
    "del cvectorizer\n",
    "print('  initial vocabulary size: {}'.format(v_size)) # initial vocabulary size: 19148\n",
    "\n",
    "# sort words in vocabulary, which put the frequent words first \n",
    "idx_sort = np.argsort(sum_counts_np) # return index of original vocab that would be used for sorting the dictionary later\n",
    "vocab_aux = [id2word[idx_sort[cc]] for cc in range(v_size)] # return a list of sorted terms\n",
    "\n",
    "# filter out stopwords (if any)\n",
    "# read stopwords\n",
    "with open('stops.txt', 'r') as f:\n",
    "    stops = f.read().split('\\n')\n",
    "\n",
    "vocab_aux = [w for w in vocab_aux if w not in stops]\n",
    "print('  vocabulary size after removing stopwords from list: {}'.format(len(vocab_aux)))\n",
    "\n",
    "# create dictionary and inverse dictionary\n",
    "vocab = vocab_aux\n",
    "del vocab_aux\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Split Corpus into Training/Validation/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing documents and splitting into train/test/valid...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' example\\n>>> np.random.permutation(10)\\narray([1, 7, 4, 3, 0, 9, 2, 5, 8, 6]) # random\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split in train/test/valid\n",
    "print('tokenizing documents and splitting into train/test/valid...')\n",
    "num_docs_tr = len(init_docs_tr)\n",
    "trSize = num_docs_tr-100 # training size\n",
    "tsSize = len(init_docs_ts) # testing size\n",
    "vaSize = 100 # validation size\n",
    "# np.random.permutation randomly generates a list of indexes (i.e., idx_permute)\n",
    "# idx_permute is then used for split training, validation , and testing dataset\n",
    "idx_permute = np.random.permutation(num_docs_tr).astype(int)\n",
    "\"\"\" example\n",
    ">>> np.random.permutation(10)\n",
    "array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6]) # random\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vocabulary after removing words not in train: 18626\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nvocabulary after removing words not in train: 18625\\n  number of documents (train): 11214 [this should be equal to 11214]\\n  number of documents (test): 7532 [this should be equal to 7532]\\n  number of documents (valid): 100 [this should be equal to 100]\\n\\nThe format:\\n    docs_tr: [[token1_id, token2_id, ...], ...]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove words not in train_data\n",
    "# as you can see, idx_permute[idx_d] is used for picking the document in init_docs\n",
    "vocab = list(set([w for idx_d in range(trSize) for w in init_docs[idx_permute[idx_d]].split() if w in word2id]))\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "print('  vocabulary after removing words not in train: {}'.format(len(vocab)))\n",
    "\n",
    "# split in train/test/valid\n",
    "docs_tr = [[word2id[w] for w in init_docs[idx_permute[idx_d]].split() if w in word2id] for idx_d in range(trSize)]\n",
    "docs_va = [[word2id[w] for w in init_docs[idx_permute[idx_d+trSize]].split() if w in word2id] for idx_d in range(vaSize)]\n",
    "docs_ts = [[word2id[w] for w in init_docs[idx_d+num_docs_tr].split() if w in word2id] for idx_d in range(tsSize)] # no permutation on the test data\n",
    "\n",
    "print('  number of documents (train): {} [this should be equal to {}]'.format(len(docs_tr), trSize))\n",
    "print('  number of documents (test): {} [this should be equal to {}]'.format(len(docs_ts), tsSize))\n",
    "print('  number of documents (valid): {} [this should be equal to {}]'.format(len(docs_va), vaSize))\n",
    "\n",
    "# remove empty documents\n",
    "print('removing empty documents...')\n",
    "\n",
    "def remove_empty(in_docs):\n",
    "    return [doc for doc in in_docs if doc!=[]]\n",
    "\n",
    "docs_tr = remove_empty(docs_tr)\n",
    "docs_ts = remove_empty(docs_ts)\n",
    "docs_va = remove_empty(docs_va)\n",
    "\n",
    "# remove test documents with length=1\n",
    "docs_ts = [doc for doc in docs_ts if len(doc)>1]\n",
    "\n",
    "# split test set in 2 halves \n",
    "# this is required input for the document completion task\n",
    "print('splitting test documents in 2 halves...')\n",
    "docs_ts_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in docs_ts]\n",
    "docs_ts_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in docs_ts]\n",
    "\n",
    "\"\"\"\n",
    "vocabulary after removing words not in train: 18625\n",
    "  number of documents (train): 11214 [this should be equal to 11214]\n",
    "  number of documents (test): 7532 [this should be equal to 7532]\n",
    "  number of documents (valid): 100 [this should be equal to 100]\n",
    "\n",
    "The format:\n",
    "    docs_tr: [[token1_id, token2_id, ...], ...]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Create Bag-of-word using sparse.coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating lists of words...\n",
      "  len(words_tr):  1339507\n",
      "  len(words_ts):  860828\n",
      "  len(words_ts_h1):  428566\n",
      "  len(words_ts_h2):  432262\n",
      "  len(words_va):  9599\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n"
     ]
    }
   ],
   "source": [
    "# getting lists of words and doc_indices\n",
    "print('creating lists of words...')\n",
    "\n",
    "def create_list_words(in_docs):\n",
    "    return [word for doc in in_docs for word in doc]\n",
    "\n",
    "words_tr = create_list_words(docs_tr)\n",
    "words_ts = create_list_words(docs_ts)\n",
    "words_ts_h1 = create_list_words(docs_ts_h1)\n",
    "words_ts_h2 = create_list_words(docs_ts_h2)\n",
    "words_va = create_list_words(docs_va)\n",
    "\n",
    "print('  len(words_tr): ', len(words_tr))\n",
    "print('  len(words_ts): ', len(words_ts))\n",
    "print('  len(words_ts_h1): ', len(words_ts_h1))\n",
    "print('  len(words_ts_h2): ', len(words_ts_h2))\n",
    "print('  len(words_va): ', len(words_va))\n",
    "\n",
    "# get doc indices\n",
    "print('getting doc indices...')\n",
    "\n",
    "def create_doc_indices(in_docs):\n",
    "    # j is the document index\n",
    "    # replicate j by len(doc) times\n",
    "    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n",
    "    return [int(index) for index_list in aux for index in index_list]\n",
    "\n",
    "doc_indices_tr = create_doc_indices(docs_tr)\n",
    "doc_indices_ts = create_doc_indices(docs_ts)\n",
    "doc_indices_ts_h1 = create_doc_indices(docs_ts_h1)\n",
    "doc_indices_ts_h2 = create_doc_indices(docs_ts_h2)\n",
    "doc_indices_va = create_doc_indices(docs_va)\n",
    "\n",
    "print('  len(np.unique(doc_indices_tr)): {} [this should be {}]'.format(len(np.unique(doc_indices_tr)), len(docs_tr)))\n",
    "print('  len(np.unique(doc_indices_ts)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts)), len(docs_ts)))\n",
    "print('  len(np.unique(doc_indices_ts_h1)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h1)), len(docs_ts_h1)))\n",
    "print('  len(np.unique(doc_indices_ts_h2)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h2)), len(docs_ts_h2)))\n",
    "print('  len(np.unique(doc_indices_va)): {} [this should be {}]'.format(len(np.unique(doc_indices_va)), len(docs_va)))\n",
    "\n",
    "# Number of documents in each set\n",
    "n_docs_tr = len(docs_tr)\n",
    "n_docs_ts = len(docs_ts)\n",
    "n_docs_ts_h1 = len(docs_ts_h1)\n",
    "n_docs_ts_h2 = len(docs_ts_h2)\n",
    "n_docs_va = len(docs_va)\n",
    "\n",
    "# Remove unused variables\n",
    "del docs_tr\n",
    "del docs_ts\n",
    "del docs_ts_h1\n",
    "del docs_ts_h2\n",
    "del docs_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "860828"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_indices_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n"
     ]
    }
   ],
   "source": [
    "# Create bow representation\n",
    "print('creating bow representation...')\n",
    "\n",
    "def create_bow(doc_indices, words, n_docs, vocab_size):\n",
    "    \"\"\" this function helps build document-term matrix in a scipy.sparse matrix format\n",
    "    API Reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html\n",
    "\n",
    "    for simplicity,\n",
    "        coo_matrix((data, (i, j)), [shape=(M, N)]) generates A\n",
    "        where  A[i[k], j[k]] = data[k]\n",
    "    note,\n",
    "        COO is a fast format for constructing sparse matrices\n",
    "        Once a matrix has been constructed, convert to CSR or CSC format for fast arithmetic and matrix vector operations\n",
    "        by default when converting to CSR or CSC format, duplicate (i,j) entries will be summed together\n",
    "    \"\"\"\n",
    "    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n",
    "\n",
    "bow_tr = create_bow(doc_indices_tr, words_tr, n_docs_tr, len(vocab))\n",
    "bow_ts = create_bow(doc_indices_ts, words_ts, n_docs_ts, len(vocab))\n",
    "bow_ts_h1 = create_bow(doc_indices_ts_h1, words_ts_h1, n_docs_ts_h1, len(vocab))\n",
    "bow_ts_h2 = create_bow(doc_indices_ts_h2, words_ts_h2, n_docs_ts_h2, len(vocab))\n",
    "bow_va = create_bow(doc_indices_va, words_va, n_docs_va, len(vocab))\n",
    "\n",
    "del words_tr\n",
    "del words_ts\n",
    "del words_ts_h1\n",
    "del words_ts_h2\n",
    "del words_va\n",
    "del doc_indices_tr\n",
    "del doc_indices_ts\n",
    "del doc_indices_ts_h1\n",
    "del doc_indices_ts_h2\n",
    "del doc_indices_va\n",
    "\n",
    "# Write the vocabulary to a file\n",
    "path_save = './min_df_' + str(min_df) + '/'\n",
    "if not os.path.isdir(path_save):\n",
    "    os.system('mkdir -p ' + path_save)\n",
    "\n",
    "with open(path_save + 'vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "del vocab\n",
    "\n",
    "# Split bow intro token/value pairs\n",
    "print('splitting bow intro token/value pairs and saving to disk...')\n",
    "\n",
    "def split_bow(bow_in, n_docs):\n",
    "    indices = [[w for w in bow_in[doc,:].indices] for doc in range(n_docs)]\n",
    "    counts = [[c for c in bow_in[doc,:].data] for doc in range(n_docs)]\n",
    "    return indices, counts\n",
    "\n",
    "bow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_docs_tr)\n",
    "savemat(path_save + 'bow_tr_tokens', {'tokens': bow_tr_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_tr_counts', {'counts': bow_tr_counts}, do_compression=True)\n",
    "del bow_tr\n",
    "del bow_tr_tokens\n",
    "del bow_tr_counts\n",
    "\n",
    "bow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_docs_ts)\n",
    "savemat(path_save + 'bow_ts_tokens', {'tokens': bow_ts_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_counts', {'counts': bow_ts_counts}, do_compression=True)\n",
    "del bow_ts\n",
    "del bow_ts_tokens\n",
    "del bow_ts_counts\n",
    "\n",
    "bow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_docs_ts_h1)\n",
    "savemat(path_save + 'bow_ts_h1_tokens', {'tokens': bow_ts_h1_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h1_counts', {'counts': bow_ts_h1_counts}, do_compression=True)\n",
    "del bow_ts_h1\n",
    "del bow_ts_h1_tokens\n",
    "del bow_ts_h1_counts\n",
    "\n",
    "bow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_docs_ts_h2)\n",
    "savemat(path_save + 'bow_ts_h2_tokens', {'tokens': bow_ts_h2_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h2_counts', {'counts': bow_ts_h2_counts}, do_compression=True)\n",
    "del bow_ts_h2\n",
    "del bow_ts_h2_tokens\n",
    "del bow_ts_h2_counts\n",
    "\n",
    "bow_va_tokens, bow_va_counts = split_bow(bow_va, n_docs_va)\n",
    "savemat(path_save + 'bow_va_tokens', {'tokens': bow_va_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_va_counts', {'counts': bow_va_counts}, do_compression=True)\n",
    "del bow_va\n",
    "del bow_va_tokens\n",
    "del bow_va_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('jigsaw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "736947d643403fbe4d8b7b20cb8243e43045620b21e488a807c0fa3a90501217"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
